//===-- IR/VPIntrinsics.def - Describes llvm.vp.* Intrinsics -*- C++ -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file contains descriptions of the various Vector Predication intrinsics.
// This is used as a central place for enumerating the different instructions
// and should eventually be the place to put comments about the instructions.
//
//===----------------------------------------------------------------------===//

// NOTE: NO INCLUDE GUARD DESIRED!

// Provide definitions of macros so that users of this file do not have to
// define everything to use it...
//
// Register a VP intrinsic and begin its property scope.
// All VP intrinsic scopes are top level, ie it is illegal to place a
// BEGIN_REGISTER_VP_INTRINSIC within a VP intrinsic scope.
// \p VPID     The VP intrinsic id.
// \p MASKPOS  The mask operand position.
// \p EVLPOS   The explicit vector length operand position.
#ifndef BEGIN_REGISTER_VP_INTRINSIC
#define BEGIN_REGISTER_VP_INTRINSIC(VPID, MASKPOS, EVLPOS)
#endif

#ifndef END_REGISTER_VP_INTRINSIC
#define END_REGISTER_VP_INTRINSIC(VPID)
#endif

#ifndef END_REGISTER_VP_SDNODE
#define END_REGISTER_VP_SDNODE(VPISD)
#endif

// This is a reduction intrinsic with accumulator arg at ACCUPOS, reduced vector
// arg at VECTORPOS.
#ifndef HANDLE_VP_REDUCTION
#define HANDLE_VP_REDUCTION(ACCUPOS, VECTORPOS, SCALAROC, SCALARINTRIN, SCALARISD)
#endif

// The intrinsic VPID of llvm.vp.* functionally corresponds to the intrinsic
// CFPID of llvm.experimental.constrained.*.
#ifndef HANDLE_VP_TO_CONSTRAINED_INTRIN
#define HANDLE_VP_TO_CONSTRAINED_INTRIN(CPFID)
#endif

// This VP intrinsic has constraint fp params.
// Rounding mode arg pos is ROUNDPOS, exception behavior arg pos is EXCEPT POS.
#ifndef HANDLE_VP_FPCONSTRAINT
#define HANDLE_VP_FPCONSTRAINT(ROUNDPOS, EXCEPTPOS)
#endif

// End the property scope of a VP intrinsic.
#ifndef END_REGISTER_VP_INTRINSIC
#define END_REGISTER_VP_INTRINSIC(VPID)
#endif

// Map this VP intrinsic to its cannonical functional intrinsic.
#ifndef HANDLE_VP_TO_INTRIN
#define HANDLE_VP_TO_INTRIN(ID)
#endif

#ifndef HANDLE_VP_IS_UNARY
#define HANDLE_VP_IS_UNARY
#endif

#ifndef HANDLE_VP_IS_BINARY
#define HANDLE_VP_IS_BINARY
#endif

#ifndef HANDLE_VP_IS_TERNARY
#define HANDLE_VP_IS_TERNARY
#endif

// This VP Intrinsic is a comparison
// (only count data params)
#ifndef HANDLE_VP_IS_XCMP
#define HANDLE_VP_IS_XCMP
#endif

// link this VP ID to a relaxed (fp semantics) VP OC
#ifndef HANDLE_VP_TO_RELAXEDSD
#define HANDLE_VP_TO_RELAXEDSD(RELAXEDSD)
#endif

///// REGISTER macros /////
// These always have to come first
// This VP Intrinsic is a memory operation
// The pointer arg is at POINTERPOS and the data arg is at DATAPOS.
#ifndef HANDLE_VP_IS_MEMOP
#define HANDLE_VP_IS_MEMOP(POINTERPOS, DATAPOS)
#endif

/// This VP Intrinsic lowers to this VP SDNode.
#ifndef HANDLE_VP_TO_SDNODE
#define HANDLE_VP_TO_SDNODE(VPISD)
#endif

// helper macro to end VP brackes with a 1:1 ISD<>IntrinsicID mapping
#define END_REGISTER_CASES(VPID, VPISD)                                        \
  HANDLE_VP_TO_SDNODE(VPISD)                                                   \
  END_REGISTER_VP_INTRINSIC(VPID)                                              \
  END_REGISTER_VP_SDNODE(VPISD)

// Register a new VP SDNode and begin its property scope.
// When the SDNode scope is nested within a VP intrinsic scope, it is implicitly registered as the canonical SDNode for this VP intrinsic.
// There is one VP intrinsic that maps directly to one SDNode that goes by the
// same name.  Since the operands are also the same, we open the property
// scopes for both the VPIntrinsic and the SDNode at once.
// \p SDOPC     The SelectionDAG Node id (eg VP_ADD).
// \p LEGALPOS The operand position of the SDNode that is used for legalizing
//             this SDNode. This can be `-1`, in which case the return type of
//             the SDNode is used.
// \p TDNAME   The name of the TableGen definition of this SDNode.
// \p MASKPOS  The mask operand position.
// \p EVLPOS   The explicit vector length operand position.
#ifndef BEGIN_REGISTER_VP_SDNODE
#define BEGIN_REGISTER_VP_SDNODE(SDOPC, LEGALPOS, TDNAME, MASKPOS, EVLPOS)
#endif

// End the property scope of a new VP SDNode.
#ifndef END_REGISTER_VP_SDNODE
#define END_REGISTER_VP_SDNODE(SDOPC)
#endif

// Helper macros for the common "1:1 - Intrinsic : SDNode" case.
//
// There is one VP intrinsic that maps directly to one SDNode that goes by the
// same name.  Since the operands are also the same, we open the property
// scopes for both the VPIntrinsic and the SDNode at once.
//
// \p INTRIN   The canonical name (eg `vp_add`, which at the same time is the
//             name of the intrinsic and the TableGen def of the SDNode).
// \p MASKPOS  The mask operand position.
// \p EVLPOS   The explicit vector length operand position.
// \p SDOPC    The SelectionDAG Node id (eg VP_ADD).
// \p LEGALPOS The operand position of the SDNode that is used for legalizing
//             this SDNode. This can be `-1`, in which case the return type of
//             the SDNode is used.
#define BEGIN_REGISTER_VP(INTRIN, MASKPOS, EVLPOS, SDOPC, LEGALPOS) \
BEGIN_REGISTER_VP_INTRINSIC(INTRIN, MASKPOS, EVLPOS) \
BEGIN_REGISTER_VP_SDNODE(SDOPC, LEGALPOS, INTRIN, MASKPOS, EVLPOS)

#define END_REGISTER_VP(INTRIN, SDOPC) \
END_REGISTER_VP_INTRINSIC(INTRIN) \
END_REGISTER_VP_SDNODE(SDOPC)


// The following macros attach properties to the scope they are placed in. This
// assigns the property to the VP Intrinsic and/or SDNode that belongs to the
// scope.
//
// Property Macros {

// The intrinsic and/or SDNode has the same function as this LLVM IR Opcode.
// \p OPC  The standard IR opcode.
#ifndef HANDLE_VP_TO_OPC
#define HANDLE_VP_TO_OPC(OPC)
#endif

/// } Property Macros

///// Integer Arithmetic {

// Specialized helper macro for integer binary operators (%x, %y, %mask, %evl).
#ifdef HELPER_REGISTER_BINARY_INT_VP
#error "The internal helper macro HELPER_REGISTER_BINARY_INT_VP is already defined!"
#endif

#define HELPER_REGISTER_BINARY_INT_VP(INTRIN, SDOPC, OPC) \
BEGIN_REGISTER_VP(INTRIN, 2, 3, SDOPC, -1) \
HANDLE_VP_TO_OPC(OPC) \
END_REGISTER_VP(INTRIN, SDOPC)

HELPER_REGISTER_BINARY_INT_VP(vp_add, VP_ADD, Add)

// llvm.vp.and(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_and, VP_AND, And)

// llvm.vp.ashr(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_ashr, VP_ASHR, AShr)

// llvm.vp.lshr(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_lshr, VP_LSHR, LShr)

// llvm.vp.mul(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_mul, VP_MUL, Mul)

// llvm.vp.or(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_or, VP_OR, Or)

// llvm.vp.sdiv(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_sdiv, VP_SDIV, SDiv)

// llvm.vp.shl(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_shl, VP_SHL, Shl)

// llvm.vp.srem(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_srem, VP_SREM, SRem)

// llvm.vp.sub(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_sub, VP_Sub, Sub)

// llvm.vp.udiv(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_udiv, VP_UDIV, UDiv)

// llvm.vp.urem(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_urem, VP_UREM, URem)

// llvm.vp.xor(x,y,mask,vlen)
HELPER_REGISTER_BINARY_INT_VP(vp_xor, VP_XOR, Xor)

#undef HELPER_REGISTER_BINARY_INT_VP

///// } Integer Arithmetic

///// Integer Arithmetic /////

// llvm.vp.ctpop(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_ctpop, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_CTPOP, -1, vp_ctpop, 1, 2)
HANDLE_VP_TO_INTRIN(ctpop)
HANDLE_VP_IS_UNARY
END_REGISTER_CASES(vp_ctpop, VP_CTPOP)

///// FP Arithmetic /////

// llvm.vp.fadd(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fadd, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FADD, -1, vp_fadd, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fadd)
HANDLE_VP_TO_OPC(FAdd)
HANDLE_VP_IS_BINARY
END_REGISTER_CASES(vp_fadd, VP_FADD)

// llvm.vp.fdiv(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fdiv, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FDIV, -1, vp_fdiv, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fdiv)
HANDLE_VP_TO_OPC(FDiv)
HANDLE_VP_IS_BINARY
END_REGISTER_CASES(vp_fdiv, VP_FDIV)

// llvm.vp.fmul(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fmul, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FMUL, -1, vp_fmul, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fmul)
HANDLE_VP_TO_OPC(FMul)
HANDLE_VP_IS_BINARY
END_REGISTER_CASES(vp_fmul, VP_FMUL)

// llvm.vp.fneg(x,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fneg, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_FNEG, -1, vp_fneg, 1, 2)
HANDLE_VP_FPCONSTRAINT(None, 1)
HANDLE_VP_TO_OPC(FNeg)
HANDLE_VP_IS_UNARY
END_REGISTER_CASES(vp_fneg, VP_FNEG)

// llvm.vp.frem(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_frem, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FREM, -1, vp_frem, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_frem)
HANDLE_VP_TO_OPC(FRem)
HANDLE_VP_IS_BINARY
END_REGISTER_CASES(vp_frem, VP_FREM)

// llvm.vp.fsub(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fsub, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FSUB, -1, vp_fsub, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fsub)
HANDLE_VP_TO_OPC(FSub)
HANDLE_VP_IS_BINARY
END_REGISTER_CASES(vp_fsub, VP_FSUB)

// llvm.vp.fma(x,y,z.round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fma, 5, 6)
BEGIN_REGISTER_VP_SDNODE(VP_FMA, -1, vp_fma, 3, 4)
HANDLE_VP_FPCONSTRAINT(3, 4)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fma)
HANDLE_VP_TO_INTRIN(fma)
HANDLE_VP_IS_TERNARY
END_REGISTER_CASES(vp_fma, VP_FMA)

///// Cast, Extend & Round /////

// llvm.vp.ceil(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_ceil, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FCEIL, -1, vp_fceil, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_ceil)
HANDLE_VP_TO_INTRIN(ceil)
END_REGISTER_CASES(vp_ceil, VP_FCEIL)

// llvm.vp.trunc(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_trunc, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FTRUNC, -1, vp_ftrunc, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_trunc)
HANDLE_VP_TO_INTRIN(trunc)
END_REGISTER_CASES(vp_trunc, VP_FTRUNC)

// llvm.vp.floor(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_floor, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FFLOOR, -1, vp_ffloor, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_floor)
HANDLE_VP_TO_INTRIN(floor)
END_REGISTER_CASES(vp_floor, VP_FFLOOR)

// llvm.vp.fpext(x,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fpext, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_FP_EXTEND, -1, vp_fpext, 1, 2)
HANDLE_VP_FPCONSTRAINT(None, 1)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fpext)
HANDLE_VP_TO_OPC(FPExt)
END_REGISTER_CASES(vp_fpext, VP_FP_EXTEND)

// llvm.vp.fptrunc(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fptrunc, 3, 4)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fptrunc)
HANDLE_VP_TO_OPC(FPTrunc)
END_REGISTER_CASES(vp_fptrunc, VP_FTRUNC)

// llvm.vp.fptoui(x,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fptoui, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_FP_TO_UINT, -1, vp_fp_to_uint, 1, 2)
HANDLE_VP_FPCONSTRAINT(None, 1)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fptoui)
HANDLE_VP_TO_OPC(FPToUI)
END_REGISTER_CASES(vp_fptoui, VP_FP_TO_UINT)

// llvm.vp.fptosi(x,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fptosi, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_FP_TO_SINT, -1, vp_fp_to_sint, 1, 2)
HANDLE_VP_FPCONSTRAINT(None, 1)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_fptosi)
HANDLE_VP_TO_OPC(FPToSI)
END_REGISTER_CASES(vp_fptosi, VP_FP_TO_SINT)

// llvm.vp.uitofp(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_uitofp, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_UINT_TO_FP, -1, vp_uint_to_fp, 1, 2)
HANDLE_VP_TO_OPC(UIToFP)
HANDLE_VP_FPCONSTRAINT(1, 2)
END_REGISTER_CASES(vp_uitofp, VP_UINT_TO_FP)
// HANDLE_VP_TO_CONSTRAINED_INTRIN(vp_uitofp,experimental_constrained_uitofp)

// llvm.vp.sitofp(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_sitofp, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_SINT_TO_FP, -1, vp_sint_to_fp, 1, 2)
HANDLE_VP_TO_OPC(SIToFP)
HANDLE_VP_FPCONSTRAINT(1, 2)
END_REGISTER_CASES(vp_sitofp, VP_SINT_TO_FP)
// HANDLE_VP_TO_CONSTRAINED_INTRIN(vp_sitofp,experimental_constrained_sitofp)

// llvm.vp.round(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_round, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FROUND, -1, vp_fround, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_round)
HANDLE_VP_TO_INTRIN(round)
END_REGISTER_CASES(vp_round, VP_FROUND)

// llvm.vp.rint(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_rint, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FRINT, -1, vp_frint, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_rint)
HANDLE_VP_TO_INTRIN(rint)
END_REGISTER_CASES(vp_rint, VP_FRINT)

// llvm.vp.nearbyint(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_nearbyint, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FNEARBYINT, -1, vp_fnearbyint, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_nearbyint)
HANDLE_VP_TO_INTRIN(nearbyint)
END_REGISTER_CASES(vp_nearbyint, VP_FNEARBYINT)

///// Math Funcs /////

// llvm.vp.sqrt(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_sqrt, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FSQRT, -1, vp_fsqrt, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_sqrt)
HANDLE_VP_TO_INTRIN(sqrt)
END_REGISTER_CASES(vp_sqrt, VP_FSQRT)

// llvm.vp.pow(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_pow, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FPOW, -1, vp_fpow, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_pow)
HANDLE_VP_TO_INTRIN(pow)
END_REGISTER_CASES(vp_pow, VP_FPOW)

// llvm.vp.powi(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_powi, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FPOWI, -1, vp_fpowi, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_powi)
HANDLE_VP_TO_INTRIN(powi)
END_REGISTER_CASES(vp_powi, VP_FPOWI)

// llvm.vp.maxnum(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_maxnum, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FMAXNUM, -1, vp_fmaxnum, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_maxnum)
HANDLE_VP_TO_INTRIN(maxnum)
END_REGISTER_CASES(vp_maxnum, VP_FMAXNUM)

// llvm.vp.minnum(x,y,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_minnum, 4, 5)
BEGIN_REGISTER_VP_SDNODE(VP_FMINNUM, -1, vp_fminnum, 2, 3)
HANDLE_VP_FPCONSTRAINT(2, 3)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_minnum)
HANDLE_VP_TO_INTRIN(minnum)
END_REGISTER_CASES(vp_minnum, VP_FMINNUM)

// llvm.vp.sin(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_sin, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FSIN, -1, vp_fsin, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_sin)
HANDLE_VP_TO_INTRIN(sin)
END_REGISTER_CASES(vp_sin, VP_FSIN)

// llvm.vp.cos(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_cos, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FCOS, -1, vp_fcos, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_cos)
HANDLE_VP_TO_INTRIN(cos)
END_REGISTER_CASES(vp_cos, VP_FCOS)

// llvm.vp.log(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_log, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FLOG, -1, vp_flog, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_log)
HANDLE_VP_TO_INTRIN(log)
END_REGISTER_CASES(vp_log, VP_FLOG)

// llvm.vp.log10(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_log10, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FLOG10, -1, vp_flog10, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_log10)
HANDLE_VP_TO_INTRIN(log10)
END_REGISTER_CASES(vp_log10, VP_FLOG10)

// llvm.vp.log2(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_log2, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FLOG2, -1, vp_flog2, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_log2)
HANDLE_VP_TO_INTRIN(log2)
END_REGISTER_CASES(vp_log2, VP_FLOG2)

// llvm.vp.exp(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_exp, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FEXP, -1, vp_fexp, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_exp)
HANDLE_VP_TO_INTRIN(exp)
END_REGISTER_CASES(vp_exp, VP_FEXP)

// llvm.vp.exp2(x,round,except,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_exp2, 3, 4)
BEGIN_REGISTER_VP_SDNODE(VP_FEXP2, -1, vp_fexp2, 1, 2)
HANDLE_VP_FPCONSTRAINT(1, 2)
HANDLE_VP_TO_CONSTRAINED_INTRIN(experimental_constrained_exp2)
HANDLE_VP_TO_INTRIN(exp2)
END_REGISTER_CASES(vp_exp2, VP_FEXP2)

///// Comparison /////

// llvm.vp.fcmp(x,y,pred,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_fcmp, 3, 4)
HANDLE_VP_TO_OPC(FCmp)
HANDLE_VP_IS_XCMP
END_REGISTER_VP_INTRINSIC(vp_fcmp)

// llvm.vp.icmp(x,y,cmp_pred,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_icmp, 3, 4)
HANDLE_VP_TO_OPC(ICmp)
HANDLE_VP_IS_XCMP
END_REGISTER_VP_INTRINSIC(vp_icmp)

// VP_SETCC (ISel only)
BEGIN_REGISTER_VP_SDNODE(VP_SETCC, -1, vp_setcc, 3, 4)
END_REGISTER_VP_SDNODE(VP_SETCC)

///// Memory Operations /////

// llvm.vp.store(ptr,val,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_store, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_STORE, 1, vp_store, 3, 4)
HANDLE_VP_TO_OPC(Store)
HANDLE_VP_TO_INTRIN(masked_store)
HANDLE_VP_IS_MEMOP(1, 0)
END_REGISTER_CASES(vp_store, VP_STORE)

// llvm.vp.scatter(ptr,val,mask,vlen)
// VP gather and scatter - load and store operations for a vector of
// random addresses with additional mask and vector length operand that
// prevents memory accesses to the masked-off lanes.
//
// Val, OutChain = VP_GATHER(InChain, BasePtr, Index, Scale, Mask, EVL)
// OutChain = VP_SCATTER(InChain, Value, BasePtr, Index, Scale, Mask, EVL)
//
// The Index operand can have more vector elements than the other operands
// due to type legalization. The extra elements are ignored.

BEGIN_REGISTER_VP_INTRINSIC(vp_scatter, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_SCATTER, 1, vp_scatter, 3, 4)
HANDLE_VP_TO_INTRIN(masked_scatter)
HANDLE_VP_IS_MEMOP(1, 0)
END_REGISTER_CASES(vp_scatter, VP_SCATTER)

// llvm.vp.load(ptr,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_load, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_LOAD, -1, vp_load, 2, 3)
HANDLE_VP_TO_OPC(Load)
HANDLE_VP_TO_INTRIN(masked_load)
HANDLE_VP_IS_MEMOP(0, None)
END_REGISTER_CASES(vp_load, VP_LOAD)

// llvm.vp.gather(ptr,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_gather, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_GATHER, -1, vp_gather, 1, 2)
HANDLE_VP_TO_INTRIN(masked_gather)
HANDLE_VP_IS_MEMOP(0, None)
END_REGISTER_CASES(vp_gather, VP_GATHER)

///// Shuffle & Blend /////

// llvm.vp.compress(x,mask,vlen)
/// VP_COMPRESS(VEC1, MASK, VLEN) - Returns a vector, of the same type as
/// VEC1.
BEGIN_REGISTER_VP_INTRINSIC(vp_compress, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_COMPRESS, -1, vp_compress, 1, 2)
END_REGISTER_CASES(vp_compress, VP_COMPRESS)

// llvm.vp.expand(x,mask,vlen)
/// VP_EXPAND(VEC1, MASK, VLEN) - Returns a vector, of the same type as
/// VEC1.
BEGIN_REGISTER_VP_INTRINSIC(vp_expand, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_EXPAND, -1, vp_expand, 1, 2)
END_REGISTER_CASES(vp_expand, VP_EXPAND)

// llvm.vp.vshift(x,amount,mask,vlen)
/// VP_VSHIFT(VEC1, AMOUNT, MASK, VLEN) - Returns a vector, of the same type as
/// VEC1. AMOUNT is an integer value. The returned vector is equivalent
/// to VEC1 shifted by AMOUNT (RETURNED_VEC[idx] = VEC1[idx + AMOUNT]).
BEGIN_REGISTER_VP_INTRINSIC(vp_vshift, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_VSHIFT, -1, vp_vshift, 2, 3)
END_REGISTER_CASES(vp_vshift, VP_VSHIFT)

// llvm.vp.select(mask,on_true,on_false,pivot,avl)
BEGIN_REGISTER_VP_INTRINSIC(vp_select, 0, 4)
BEGIN_REGISTER_VP_SDNODE(VP_SELECT, -1, vp_select, 0, 4)
END_REGISTER_CASES(vp_select, VP_SELECT)

///// Reduction /////

// llvm.vp.reduce.add(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_add, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_ADD, 0, vp_reduce_add, 1, 2)
HANDLE_VP_REDUCTION(None, 0, ADD, None, ADD)
HANDLE_VP_TO_INTRIN(vector_reduce_add)
END_REGISTER_CASES(vp_reduce_add, VP_REDUCE_ADD)

// llvm.vp.reduce.mul(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_mul, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_MUL, 0, vp_reduce_mul, 1, 2)
HANDLE_VP_REDUCTION(None, 0, MUL, None, MUL)
HANDLE_VP_TO_INTRIN(vector_reduce_mul)
END_REGISTER_CASES(vp_reduce_mul, VP_REDUCE_MUL)

// llvm.vp.reduce.and(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_and, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_AND, 0, vp_reduce_and, 1, 2)
HANDLE_VP_REDUCTION(None, 0, AND, None, AND)
HANDLE_VP_TO_INTRIN(vector_reduce_and)
END_REGISTER_CASES(vp_reduce_and, VP_REDUCE_AND)

// llvm.vp.reduce.or(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_or, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_OR, 0, vp_reduce_or, 1, 2)
HANDLE_VP_REDUCTION(None, 0, OR, None, OR)
HANDLE_VP_TO_INTRIN(vector_reduce_or)
END_REGISTER_CASES(vp_reduce_or, VP_REDUCE_OR)

// llvm.vp.reduce.xor(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_xor, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_XOR, 0, vp_reduce_xor, 1, 2)
HANDLE_VP_REDUCTION(None, 0, XOR, None, XOR)
HANDLE_VP_TO_INTRIN(vector_reduce_xor)
END_REGISTER_CASES(vp_reduce_xor, VP_REDUCE_XOR)

// llvm.vp.reduce.smin(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_smin, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_SMIN, 0, vp_reduce_smin, 1, 2)
HANDLE_VP_REDUCTION(None, 0, None, int_smin, SMIN)
HANDLE_VP_TO_INTRIN(vector_reduce_smin)
END_REGISTER_CASES(vp_reduce_smin, VP_REDUCE_SMIN)

// llvm.vp.reduce.smax(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_smax, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_SMAX, 0, vp_reduce_smax, 1, 2)
HANDLE_VP_REDUCTION(None, 0, None, int_smax, SMAX)
HANDLE_VP_TO_INTRIN(vector_reduce_smax)
END_REGISTER_CASES(vp_reduce_smax, VP_REDUCE_SMAX)

// llvm.vp.reduce.umin(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_umin, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_UMIN, 0, vp_reduce_umin, 1, 2)
HANDLE_VP_REDUCTION(None, 0, None, int_umin, UMIN)
HANDLE_VP_TO_INTRIN(vector_reduce_umin)
END_REGISTER_CASES(vp_reduce_umin, VP_REDUCE_UMIN)

// llvm.vp.reduce.umax(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_umax, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_UMAX, 0, vp_reduce_umax, 1, 2)
HANDLE_VP_REDUCTION(None, 0, None, int_umax, UMAX)
HANDLE_VP_TO_INTRIN(vector_reduce_umax)
END_REGISTER_CASES(vp_reduce_umax, VP_REDUCE_UMAX)

// llvm.vp.reduce.fadd(accu,x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_fadd, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_SEQ_FADD, 1, vp_reduce_seq_fadd, 1, 2)
HANDLE_VP_REDUCTION(0, 1, FADD, None, FADD)
HANDLE_VP_TO_INTRIN(vector_reduce_fadd)
HANDLE_VP_TO_RELAXEDSD(VP_REDUCE_FADD)
END_REGISTER_CASES(vp_reduce_fadd, VP_REDUCE_SEQ_FADD)

// vp_reduce_fadd (relaxed fp reduction node)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_FADD, 0, vp_reduce_fadd, 2, 3)
HANDLE_VP_REDUCTION(None, 0, FADD, None, FADD)
END_REGISTER_VP_SDNODE(VP_REDUCE_FADD)

// llvm.vp.reduce.fmul(accu,x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_fmul, 2, 3)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_SEQ_FMUL, 1, vp_reduce_seq_fmul, 1, 2)
HANDLE_VP_REDUCTION(0, 1, FMUL, None, FMUL)
HANDLE_VP_TO_INTRIN(vector_reduce_fmul)
HANDLE_VP_TO_RELAXEDSD(VP_REDUCE_FMUL)
END_REGISTER_CASES(vp_reduce_fmul, VP_REDUCE_SEQ_FMUL)

// vp_strict_reduce_fmul (associated node)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_FMUL, 0, vp_reduce_fmul, 2, 3)
HANDLE_VP_REDUCTION(None, 0, FMUL, None, FMUL)
END_REGISTER_VP_SDNODE(VP_REDUCE_FMUL)

// llvm.vp.reduce.fmin(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_fmin, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_FMIN, 0, vp_reduce_fmin, 0, 1)
HANDLE_VP_REDUCTION(None, 0, None, int_minnum, FMINNUM)
HANDLE_VP_TO_INTRIN(vector_reduce_fmin)
END_REGISTER_CASES(vp_reduce_fmin, VP_REDUCE_FMIN)

// llvm.vp.reduce.fmax(x,mask,vlen)
BEGIN_REGISTER_VP_INTRINSIC(vp_reduce_fmax, 1, 2)
BEGIN_REGISTER_VP_SDNODE(VP_REDUCE_FMAX, 0, vp_reduce_fmax, 0, 1)
HANDLE_VP_REDUCTION(None, 0, None, int_maxnum, FMAXNUM)
HANDLE_VP_TO_INTRIN(vector_reduce_fmax)
END_REGISTER_CASES(vp_reduce_fmax, VP_REDUCE_FMAX)

#undef BEGIN_REGISTER_VP
#undef BEGIN_REGISTER_VP_INTRINSIC
#undef BEGIN_REGISTER_VP_SDNODE
#undef END_REGISTER_CASES
#undef END_REGISTER_VP
#undef END_REGISTER_VP_INTRINSIC
#undef END_REGISTER_VP_SDNODE
#undef HANDLE_VP_FPCONSTRAINT
#undef HANDLE_VP_IS_BINARY
#undef HANDLE_VP_IS_MEMOP
#undef HANDLE_VP_IS_TERNARY
#undef HANDLE_VP_IS_UNARY
#undef HANDLE_VP_IS_XCMP
#undef HANDLE_VP_REDUCTION
#undef HANDLE_VP_TO_CONSTRAINED_INTRIN
#undef HANDLE_VP_TO_INTRIN
#undef HANDLE_VP_TO_OPC
#undef HANDLE_VP_TO_RELAXEDSD
#undef HANDLE_VP_TO_SDNODE
