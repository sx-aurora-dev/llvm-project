; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=ve-unknown-unknown -mattr=+vpu,+packed | FileCheck %s

;;; 512 x float


; Function Attrs: nounwind
define fastcc <512 x float> @vec_load_v512f32(<512 x float>* %P) {
; CHECK-LABEL: vec_load_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s1, 256
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vldu %v0, 8, %s0
; CHECK-NEXT:    lea %s0, 4(, %s0)
; CHECK-NEXT:    vldu %v1, 8, %s0
; CHECK-NEXT:    vshf %v0, %v1, %v0, 8
; CHECK-NEXT:    b.l.t (, %s10)
  %ret = load <512 x float>, <512 x float>* %P, align 4
  ret <512 x float> %ret
}


declare <512 x float> @llvm.masked.load.v512f32.p0v512f32(<512 x float>*, i32 immarg, <512 x i1>, <512 x float>)

; Function Attrs: nounwind
define fastcc <512 x float> @vec_mload_v512f32(<512 x float>* %P, <512 x i1> %M) {
; CHECK-LABEL: vec_mload_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s1, 256
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vld %v0, 8, %s0
; CHECK-NEXT:    vshf %v0, %v0, %v0, 4
; CHECK-NEXT:    b.l.t (, %s10)
  %ret = call <512 x float> @llvm.masked.load.v512f32.p0v512f32(<512 x float>* %P, i32 16, <512 x i1> %M, <512 x float> undef)
  ret <512 x float> %ret
}

declare <512 x float> @llvm.vp.load.v512f32.p0v512f32(<512 x float>*, <512 x i1>, i32)

; Function Attrs: nounwind
define fastcc <512 x float> @vec_vpload_v512f32(<512 x float>* %P, <512 x i1> %M, i32 %avl) {
; CHECK-LABEL: vec_vpload_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    adds.w.sx %s1, 1, %s1
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vld %v0, 8, %s0
; CHECK-NEXT:    vshf %v0, %v0, %v0, 4
; CHECK-NEXT:    b.l.t (, %s10)
  %ret = call <512 x float> @llvm.vp.load.v512f32.p0v512f32(<512 x float>* %P, <512 x i1> %M, i32 %avl)
  ret <512 x float> %ret
}


;;; 512 x double


; Function Attrs: nounwind
define fastcc <512 x double> @vec_load_v512f64(<512 x double>* %P) {
; CHECK-LABEL: vec_load_v512f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s1, 8(, %s0)
; CHECK-NEXT:    lea %s2, 256
; CHECK-NEXT:    lvl %s2
; CHECK-NEXT:    vld %v1, 16, %s1
; CHECK-NEXT:    vld %v0, 16, %s0
; CHECK-NEXT:    # kill: def $v0 killed $v0 def $vp0 killed $v1
; CHECK-NEXT:    b.l.t (, %s10)
  %ret = load <512 x double>, <512 x double>* %P, align 4
  ret <512 x double> %ret
}


declare <512 x double> @llvm.masked.load.v512f64.p0v512f64(<512 x double>*, i32 immarg, <512 x i1>, <512 x double>)

; Function Attrs: nounwind
define fastcc <512 x double> @vec_mload_v512f64(<512 x double>* %P, <512 x i1> %M) {
; CHECK-LABEL: vec_mload_v512f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s1, 8(, %s0)
; CHECK-NEXT:    lea %s2, 256
; CHECK-NEXT:    lvl %s2
; CHECK-NEXT:    vld %v1, 16, %s1
; CHECK-NEXT:    vld %v0, 16, %s0
; CHECK-NEXT:    # kill: def $v0 killed $v0 def $vp0 killed $v1
; CHECK-NEXT:    b.l.t (, %s10)
  %ret = call <512 x double> @llvm.masked.load.v512f64.p0v512f64(<512 x double>* %P, i32 16, <512 x i1> %M, <512 x double> undef)
  ret <512 x double> %ret
}

declare <512 x double> @llvm.vp.load.v512f64.p0v512f64(<512 x double>*, <512 x i1>, i32)

; Function Attrs: nounwind
define fastcc <512 x double> @vec_vpload_v512f64(<512 x double>* %P, <512 x i1> %M, i32 %avl) {
; CHECK-LABEL: vec_vpload_v512f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    adds.w.sx %s1, 1, %s1
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    lea %s2, 8(, %s0)
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vld %v1, 16, %s2
; CHECK-NEXT:    vld %v0, 16, %s0
; CHECK-NEXT:    # kill: def $v0 killed $v0 def $vp0 killed $v1
; CHECK-NEXT:    b.l.t (, %s10)
  %ret = call <512 x double> @llvm.vp.load.v512f64.p0v512f64(<512 x double>* %P, <512 x i1> %M, i32 %avl)
  ret <512 x double> %ret
}

