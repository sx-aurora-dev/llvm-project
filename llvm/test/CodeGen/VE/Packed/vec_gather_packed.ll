; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=ve-unknown-unknown -mattr=+packed --ve-fast-mem=0 | FileCheck %s

;; regular

; Function Attrs: nounwind
define fastcc <128 x double> @vec_gather_v128f64(<128 x double*> %P, <128 x i1> %M) {
; CHECK-LABEL: vec_gather_v128f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s0, 128
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgt %v0, %v0, 0, 0, %vm1
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <128 x double> @llvm.masked.gather.v128f64.v128p0f64(<128 x double*> %P, i32 16, <128 x i1> %M, <128 x double> undef)
  ret <128 x double> %r
}

; Function Attrs: nounwind
define fastcc <256 x double> @vec_gather_v256f64(<256 x double*> %P, <256 x i1> %M) {
; CHECK-LABEL: vec_gather_v256f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s0, 256
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgt %v0, %v0, 0, 0, %vm1
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <256 x double> @llvm.masked.gather.v256f64.v256p0f64(<256 x double*> %P, i32 16, <256 x i1> %M, <256 x double> undef)
  ret <256 x double> %r
}

; Function Attrs: nounwind
define fastcc <256 x double> @vec_gather_pt_v256f64(<256 x double*> %P, <256 x double> %PT, <256 x i1> %M) {
; CHECK-LABEL: vec_gather_pt_v256f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s0, 256
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgt %v2, %v0, 0, 0, %vm1
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v0, (0)1, %v1
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vmrg %v0, %v1, %v2, %vm1
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <256 x double> @llvm.masked.gather.v256f64.v256p0f64(<256 x double*> %P, i32 16, <256 x i1> %M, <256 x double> %PT)
  ret <256 x double> %r
}

;;; overpacked

; Function Attrs: nounwind
define fastcc <512 x double> @vec_gather_pt_v512f64(<512 x double*> %P, <512 x double> %PT, <512 x i1> %M) {
; CHECK-LABEL: vec_gather_pt_v512f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s0, 256
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgt %v4, %v1, 0, 0, %vm3
; CHECK-NEXT:    vgt %v5, %v0, 0, 0, %vm2
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v0, (0)1, %v2
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v1, (0)1, %v3
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vmrg %v1, %v3, %v4, %vm3
; CHECK-NEXT:    vmrg %v0, %v2, %v5, %vm2
; CHECK-NEXT:    # kill: def $v0 killed $v0 def $vp0 killed $v1
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <512 x double> @llvm.masked.gather.v512f64.v512p0f64(<512 x double*> %P, i32 16, <512 x i1> %M, <512 x double> %PT)
  ret <512 x double> %r
}

; Function Attrs: nounwind
define fastcc <512 x double> @vec_gather_v512f64(<512 x double*> %P, <512 x i1> %M) {
; CHECK-LABEL: vec_gather_v512f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s0, 256
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgt %v1, %v1, 0, 0, %vm3
; CHECK-NEXT:    vgt %v0, %v0, 0, 0, %vm2
; CHECK-NEXT:    # kill: def $v0 killed $v0 def $vp0 killed $v1
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <512 x double> @llvm.masked.gather.v512f64.v512p0f64(<512 x double*> %P, i32 16, <512 x i1> %M, <512 x double> undef)
  ret <512 x double> %r
}

; Function Attrs: nounwind
define fastcc <512 x double> @vp_gather_v512f64(<512 x double*> %P, <512 x i1> %M, i32 %avl) {
; CHECK-LABEL: vp_gather_v512f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    and %s1, %s0, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vgt %v1, %v1, 0, 0, %vm3
; CHECK-NEXT:    adds.w.sx %s0, 1, %s0
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    srl %s0, %s0, 1
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgt %v0, %v0, 0, 0, %vm2
; CHECK-NEXT:    # kill: def $v0 killed $v0 def $vp0 killed $v1
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <512 x double> @llvm.vp.gather.v512f64.v512p0f64(<512 x double*> %P, <512 x i1> %M, i32 %avl)
  ret <512 x double> %r
}


;;; packed

; Function Attrs: nounwind
define fastcc <512 x float> @vec_gather_pt_v512f32(<512 x float*> %P, <512 x float> %PT, <512 x i1> %M) {
; CHECK-LABEL: vec_gather_pt_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $vm2 killed $vm2 killed $vmp1 def $vmp1
; CHECK-NEXT:    lea %s0, 256
; CHECK-NEXT:    # kill: def $vm3 killed $vm3 killed $vmp1 def $vmp1
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgtu %v3, %v0, 0, 0, %vm2
; CHECK-NEXT:    vgtu %v1, %v1, 0, 0, %vm3
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v0, (0)1, %v2
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vshf %v1, %v1, %v3, 8
; CHECK-NEXT:    vmrg.w %v0, %v2, %v1, %vm2
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <512 x float> @llvm.masked.gather.v512f32.v512p0f32(<512 x float*> %P, i32 16, <512 x i1> %M, <512 x float> %PT)
  ret <512 x float> %r
}

; Function Attrs: nounwind
define fastcc <512 x float> @vec_gather_v512f32(<512 x float*> %P, <512 x i1> %M) {
; CHECK-LABEL: vec_gather_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s0, 256
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgtu %v0, %v0, 0, 0, %vm2
; CHECK-NEXT:    vgtu %v1, %v1, 0, 0, %vm3
; CHECK-NEXT:    vshf %v0, %v1, %v0, 8
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <512 x float> @llvm.masked.gather.v512f32.v512p0f32(<512 x float*> %P, i32 16, <512 x i1> %M, <512 x float> undef)
  ret <512 x float> %r
}

; Function Attrs: nounwind
define fastcc <512 x float> @vp_gather_v512f32(<512 x float*> %P, <512 x i1> %M, i32 %avl) {
; CHECK-LABEL: vp_gather_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    and %s1, %s0, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vgtu %v1, %v1, 0, 0, %vm3
; CHECK-NEXT:    adds.w.sx %s0, 1, %s0
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    srl %s0, %s0, 1
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgtu %v0, %v0, 0, 0, %vm2
; CHECK-NEXT:    vshf %v0, %v1, %v0, 8
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call <512 x float> @llvm.vp.gather.v512f32.v512p0f32(<512 x float*> %P, <512 x i1> %M, i32 %avl)
  ret <512 x float> %r
}

; Function Attrs: nounwind
define fastcc <512 x float> @vp_gather_idx_v512f32(float* %B, <512 x i64> %I, <512 x i1> %M, i32 %avl) {
; CHECK-LABEL: vp_gather_idx_v512f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    adds.w.sx %s2, 1, %s1
; CHECK-NEXT:    and %s2, %s2, (32)0
; CHECK-NEXT:    srl %s2, %s2, 1
; CHECK-NEXT:    lvl %s2
; CHECK-NEXT:    vmuls.l %v0, 4, %v0
; CHECK-NEXT:    vadds.l %v0, %s0, %v0
; CHECK-NEXT:    vgtu %v0, %v0, 0, 0, %vm2
; CHECK-NEXT:    vmuls.l %v1, 4, %v1
; CHECK-NEXT:    vadds.l %v1, %s0, %v1
; CHECK-NEXT:    and %s0, %s1, (32)0
; CHECK-NEXT:    srl %s0, %s0, 1
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    vgtu %v1, %v1, 0, 0, %vm3
; CHECK-NEXT:    lvl %s2
; CHECK-NEXT:    vshf %v0, %v1, %v0, 8
; CHECK-NEXT:    b.l.t (, %s10)
  %P = getelementptr inbounds float, float* %B, <512 x i64> %I
  %r = call <512 x float> @llvm.vp.gather.v512f32.v512p0f32(<512 x float*> %P, <512 x i1> %M, i32 %avl)
  ret <512 x float> %r
}

declare <256 x double> @llvm.vp.gather.v256f64.v256p0f64(<256 x double*>, <256 x i1>, i32)
declare <512 x float> @llvm.vp.gather.v512f32.v512p0f32(<512 x float*>, <512 x i1>, i32)
declare <512 x double> @llvm.vp.gather.v512f64.v512p0f64(<512 x double*>, <512 x i1>, i32)

declare <512 x float> @llvm.masked.gather.v512f32.v512p0f32(<512 x float*> %0, i32 immarg %1, <512 x i1> %2, <512 x float> %3)
declare <512 x double> @llvm.masked.gather.v512f64.v512p0f64(<512 x double*> %0, i32 immarg %1, <512 x i1> %2, <512 x double> %3)
declare <256 x double> @llvm.masked.gather.v256f64.v256p0f64(<256 x double*> %0, i32 immarg %1, <256 x i1> %2, <256 x double> %3)
declare <128 x double> @llvm.masked.gather.v128f64.v128p0f64(<128 x double*> %0, i32 immarg %1, <128 x i1> %2, <128 x double> %3)
