; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O0 --march=ve -mattr=+vpu %s -o=/dev/stdout | FileCheck %s

define fastcc void @test_vp_harness(<256 x i32>* %Out, <256 x i32> %i0) {
; CHECK-LABEL: test_vp_harness:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s1, 256
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vstl %v0, 4, %s0
; CHECK-NEXT:    b.l.t (, %s10)
  store <256 x i32> %i0, <256 x i32>* %Out
  ret void
}

define fastcc void @test_vp_add_sub_mul(<256 x i32>* %Out, <256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n) {
; CHECK-LABEL: test_vp_add_sub_mul:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v2, (0)1, %v1
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v1, (0)1, %v0
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vadds.w.sx %v0, %v1, %v2
; CHECK-NEXT:    vsubs.w.sx %v1, %v1, %v2
; CHECK-NEXT:    vmuls.w.sx %v0, %v0, %v1
; CHECK-NEXT:    lea %s1, 256
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vstl %v0, 4, %s0
; CHECK-NEXT:    b.l.t (, %s10)
  %r0 = call <256 x i32> @llvm.vp.add.v256i32(<256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  %r1 = call <256 x i32> @llvm.vp.sub.v256i32(<256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  %r2 = call <256 x i32> @llvm.vp.mul.v256i32(<256 x i32> %r0, <256 x i32> %r1, <256 x i1> %m, i32 %n)
  store <256 x i32> %r2, <256 x i32>* %Out
  ret void
}

define fastcc void @test_vp_su_div(<256 x i32>* %Out, <256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n) {
; CHECK-LABEL: test_vp_su_div:
; CHECK:       # %bb.0:
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vdivs.w.sx %v0, %v0, %v1, %vm1
; CHECK-NEXT:    vdivu.w %v0, %v0, %v1, %vm1
; CHECK-NEXT:    lea %s1, 256
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vstl %v0, 4, %s0
; CHECK-NEXT:    b.l.t (, %s10)
  %r0 = call <256 x i32> @llvm.vp.sdiv.v256i32(<256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  %r1 = call <256 x i32> @llvm.vp.udiv.v256i32(<256 x i32> %r0, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  store <256 x i32> %r1, <256 x i32>* %Out
  ret void
}


define fastcc void @test_vp_bitarith(<256 x i32>* %Out, <256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n) {
; CHECK-LABEL: test_vp_bitarith:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v2, (0)1, %v1
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v1, (0)1, %v0
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    pvand.lo %v0, %v1, %v2
; CHECK-NEXT:    pvor.lo %v3, %v0, %v2
; CHECK-NEXT:    pvxor.lo %v1, %v1, %v3
; CHECK-NEXT:    pvsra.lo %v1, %v1, %v2
; CHECK-NEXT:    pvsrl.lo %v0, %v1, %v0
; CHECK-NEXT:    pvsll.lo %v0, %v0, %v1
; CHECK-NEXT:    lea %s1, 256
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vstl %v0, 4, %s0
; CHECK-NEXT:    b.l.t (, %s10)
  %r0 = call <256 x i32> @llvm.vp.and.v256i32(<256 x i32> %i0, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  %r1 = call <256 x i32> @llvm.vp.or.v256i32(<256 x i32> %r0, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  %r2 = call <256 x i32> @llvm.vp.xor.v256i32(<256 x i32> %i0, <256 x i32> %r1, <256 x i1> %m, i32 %n)
  %r3 = call <256 x i32> @llvm.vp.ashr.v256i32(<256 x i32> %r2, <256 x i32> %i1, <256 x i1> %m, i32 %n)
  %r4 = call <256 x i32> @llvm.vp.lshr.v256i32(<256 x i32> %r3, <256 x i32> %r0, <256 x i1> %m, i32 %n)
  %r5 = call <256 x i32> @llvm.vp.shl.v256i32(<256 x i32> %r4, <256 x i32> %r3, <256 x i1> %m, i32 %n)
  store <256 x i32> %r5, <256 x i32>* %Out
  ret void
}

define fastcc void @test_vp_memory(<256 x i32>* %VecPtr, <256 x i32*> %PtrVec, <256 x i32> %i0, <256 x i1> %m, i32 %n) {
; CHECK-LABEL: test_vp_memory:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lea %s16, 256
; CHECK-NEXT:    lvl %s16
; CHECK-NEXT:    vor %v1, (0)1, %v0
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    vldl.zx %v2, 4, %s0
; CHECK-NEXT:    vgtl.zx %v0, %v1, 0, 0, %vm1
; CHECK-NEXT:    vscl %v2, %v1, 0, 0, %vm1
; CHECK-NEXT:    vstl %v0, 4, %s0, %vm1
; CHECK-NEXT:    b.l.t (, %s10)
  %r0 = call <256 x i32> @llvm.vp.load.v256i32.p0v256i32(<256 x i32>* %VecPtr, <256 x i1> %m, i32 %n)
  %r1 = call <256 x i32> @llvm.vp.gather.v256i32.v256p0i32(<256 x i32*> %PtrVec, <256 x i1> %m, i32 %n)
  call void @llvm.vp.scatter.v256i32.v256p0i32(<256 x i32> %r0, <256 x i32*> %PtrVec, <256 x i1> %m, i32 %n)
  call void @llvm.vp.store.v256i32.p0v256i32(<256 x i32> %r1, <256 x i32>* %VecPtr, <256 x i1> %m, i32 %n)
  ret void
}

; integer arith
declare <256 x i32> @llvm.vp.add.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.sub.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.mul.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.sdiv.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.udiv.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
; bit arith
declare <256 x i32> @llvm.vp.and.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.xor.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.or.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.ashr.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.lshr.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.shl.v256i32(<256 x i32>, <256 x i32>, <256 x i1>, i32)

; memory
declare void @llvm.vp.store.v256i32.p0v256i32(<256 x i32>, <256 x i32>*, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.load.v256i32.p0v256i32(<256 x i32>*, <256 x i1>, i32)
declare void @llvm.vp.scatter.v256i32.v256p0i32(<256 x i32>, <256 x i32*>, <256 x i1>, i32)
declare <256 x i32> @llvm.vp.gather.v256i32.v256p0i32(<256 x i32*>, <256 x i1>, i32)
